```{r setup, cache=FALSE, include=FALSE, message=FALSE}
library(knitr)
output <- opts_knit$get("rmarkdown.pandoc.to")
if (output=="html") opts_chunk$set(fig.width=8, fig.height=8, warning=FALSE, message=FALSE, tidy=TRUE, echo=FALSE)
```
=======================================================

Scott Burns

Udacity – Data Analyst Nanodegree

Project 3: Data Analysis with R

========================================================
  
# Introduction and Background
###Why I chose this dataset

I live in the city of Oakland, California, and plan to stay here. I'm very curious about crime trends in the city, as they may have a direct impact on my life and important decisions I encounter in the future. Local crime data is also interesting to cover given the popular perception of Oakland as a relatively dangerous place to live. In exploring dataset options for this project, I came across [this dataset](http://data.openoakland.org/dataset/crime-reports/resource/49bee847-a9b7-4e71-84d8-3f4cabb26cf0) on the website OpenOakland.org - which includes all crime reports from the city of Oakland from 2007 to earlier this summer, including incident details such as geographic location and type of crime. It looked fascinating to me, and I decided to dive in.

###Questions I'd love to explore

1. How has the incidence of crime been trending in the city over the last 8 years?
2. Have incidences of particular types of crime been growing / diminishing differentially over the last few years?
3. Where is crime occurring geographically?
4. How has crime incidence changed in particular areas within the city?

Most of all, I'd like to create a few thought-provoking visualizations and see if they might suggest directions for more in-depth exploration.

###About the dataset:

The data were downloaded from [data.openoakland.org](http://data.openoakland.org/dataset/crime-reports/resource/49bee847-a9b7-4e71-84d8-3f4cabb26cf0).

Additional background on the dataset is available [on Rik Belew's blog ](http://rikiwiki.electronicartifacts.com/wp-content/uploads/2013/05/showCrime_v21.pdf).

Background on the dataset's CrimeCat classifications can be found [on this explainer page](http://rikiwiki.electronicartifacts.com/opd-crime-statistics/crimecatoak).

More detailed background on the dataset is available in Dataset Description section at the end of this file.

***

#Stream-of-Consciousness Exploration

### Getting Started: Preparing the RStudio environment

In preparation for analysis, I loaded in the dataset of interest, and glanced at summary information about it (output suppressed).

```{r To get started - reading in dataset and preparing libraries, include=FALSE}

setwd('/Users/burnssa/Google Drive/Personal Files/Education/Data Science Nanodegree/Project 3/Final Project')
getwd()
crimes <- read.csv('OPD_150708.csv')
summary(crimes) #Output suppressed for final report
head(crimes) #to illustrate the available fields, with example values

#Include libraries - all critical to the analysis
library(dplyr)
library(ggplot2)
#install.packages('ggmap')
library(ggmap)
library("reshape2")
library(RColorBrewer)
#install.packages('memisc')
library(gridExtra)
library(memisc)
```

I also created two variables to potentially use throughout subsequent plots.

1. An `any_crime` dummy variable. For this, any record where `Desc` and `CrimeCat` are not blank is assigned a 1 value. If a record doesn't even have this minimal information about the crime, I think it's better to exclude it as an incident.

```{r}

crimes$any_crime <- with(crimes, (Desc != "" & CrimeCat != "")) * 1

```

2. Add column `date_format` with date representation of `Date` string

```{r}

crimes$date_format <- as.Date(crimes$Date, format = "%m/%d/%y")

```
  
### Initial Exploration: Rough plots of crime data

As a first basic attempt to visualize the data, I plotted a histogram of crime reports, using a binwidth of 30 days.

```{r Exploration - aggregate trend in daily crime over full dataset period with date histogram}

ggplot(crimes, aes(x = date_format)) +
  geom_histogram(binwidth = 30)

```

From the histogram, we see indications that crime reports have fallen significantly in Oakland over the last 7 years, as report counts per 30-day period in 2007 and 2008 appear to number between 9,000 and 10,000, while counts per period have been around 4,000 in recent years.

I was also curious how crimes were distributed throughout the day over the period in question. To see the dynamics, I plotted another histogram by time of day, with hourly bins, creating an `hour` variable to more easily bin incidents. 

```{r Exploration - crime incidence by time of day (a)}

crimes$time_format <- as.POSIXct(strptime(crimes$Time, "%H:%M:%S")) #as.POSIXct added to address errors grouping in dplyr
crimes$hour <- as.numeric(format(crimes$time_format, "%H"))

ggplot(crimes, aes(x = hour)) +
  geom_histogram(binwidth = 1)
```

The hourly distribution suprised me - first by the huge spike at hour 0 (e.g. midnight to 1am). I suspected that this might be related to data entry issues, so I also checked the composition of crimes that were committed at hour zero, loading them from the original `Time` variable. From this, I discovered that 98,823 of the 118,590 `one_am` records have the value exactly "0:00:00". This strengthened my suspicion about the spike being driven by coding issues. 

Relatively confident that most of the 98,823 records occurring at midnight were probably set in the absence of a known time for a crime, or as a result of time recording errors, I decided to run the histogram on a subset that removed the 'exactly midnight' crimes.

```{r Exploration - crime incidence by time of day (b)}

one_am <- subset(crimes, substr(as.character(Time),1,1) == "0" )
summary(one_am$Time)

ggplot(subset(crimes, as.character(Time) != "0:00:00") , aes(x = hour)) +
  geom_histogram(binwidth = 1)

```

In the modified histogram, I was also somewhat surprised that crimes don't spike to a greater extent in the evenings, with incident rates staying fairly uniform from 10am to 4pm, then rising to a moderate peak around 7pm and declining from there to a minimum at the 5am hour.

Next, I wanted to explore the distribution of crime by latitude and longitude, with histograms for each variable. 

After a first glance at raw plots for each, I realized I needed to remove a few obviously incorrect entries, to arrive at the histograms below, and obtained more useful ranges for a chart by summarizing reasonable values for `Lat`.

```{r Exploration - histograms by longitude and latitude}

summary(subset(crimes$Lat, (crimes$Lat < 40 & crimes$Lat > 37)))
ggplot(subset(crimes, (Lat > 37.7 & Lat < 37.9)), aes(x = Lat)) +
  geom_histogram()

summary(subset(crimes$Lng, (crimes$Lng > -123 & crimes$Lng < -122.1 )))
ggplot(subset(crimes, (Lng > -122.35 & Lng < -122.15)), aes(x = Lng)) +
  geom_histogram()

crimesMapSubset <- subset(crimes, (Lat > 37.7 & Lat < 37.9) & (Lng > -122.35 & Lng < -122.15))

```

Crime seems to skew toward the mid-Northern and West parts of the city, based on the histogram.

For a more detailed view of geographic distribution, I decided to use ggmap to plot a basic heatmap of crime in Oakland for the period in question.

```{r Exploration - crime heatmap by longitude and latitude}

oakland_location <- c(-122.33, 37.71, -122.15, 37.9)
oakland_map <- get_map(location = oakland_location, source="google", maptype="hybrid")

ggmap(oakland_map) +
  geom_density2d(data = crimesMapSubset, aes(x = Lng, y = Lat), size = 1) +
  stat_density2d(data = crimesMapSubset, aes(x = Lng, y = Lat, fill = ..level.., alpha = ..level..), size = 0.01, bins = 20, geom = 'polygon') +
  scale_fill_gradient(low = "green", high = "red")

```

In the heatmap, we see a strong concentration of crimes in the downtown area, with a pocket also slightly to the northwest along San Pablo Avenue, then all along International Avenue to the southeast.

For another view of Oakland crime dynamics, I created a line plot of incidents per day, using the dplyr aggregation techniques we learned in the Data Analysis in R course.

_Note: creating new dataframe based on incidents per day, using dplyr 'verbose' method, then using this in line plot._

```{r Exploration - line plot of crime incidents per day over time}

crime_day <- group_by(crimes, date_format) 
crime_by_day <- summarize(crime_day, 
                          incidents = sum(any_crime),
                          n=n()) 

ggplot(aes(x=date_format, y=incidents), data=crime_by_day) +
  geom_line() +
  geom_smooth()

```

In the daily line plot, the decline of crime reports over time is clearly visible.

As daily crime incident counts are fairly noisy, I wanted to take a look at potentially smoother time increments - creating a line plot of `any_crime` incidents each month.

To create the monthly plot, I cut our `date_format` data in monthly units, using the syntax we learned in lesson 5 of the Data Analysis in R course, then aggregated crime data by month. To do so, I applied dplyr methods again, but this time using the 'concise' syntax.

```{r Exploration - line plot of crime incidents per month over time}

crimes$month <- as.Date(cut(crimes$date_format,
  breaks = "month"))

crime_by_month <- crimes %>%
  group_by(month) %>%
  summarize(incidents = sum(any_crime),
            n = n()) %>%
  arrange(month)

ggplot(aes(x=month, y=incidents), data=crime_by_month) +
  geom_line() +
  geom_smooth(aes(group = 1))

```

The downward trend in crime reports is again evident in plotted monthly crime report data. Another striking feature of the chart is the large vertical drop in incident count at the beginning of 2014. 

It seems very strange to me that the reported number of crimes would be fairly constant throughout 2012 and 2013, fall by around 40% as the year changed, then persist at a largely constant lower level around 3,700 incidents per month for the next year. I wonder if there was a major change in the way crimes were recorded or reported starting at the beginning of 2014.

### Initial Exploration: Rough plots with incidents segmented by crime type

After having uncovered some interesting insights about local crime trends at an aggregate level, I wanted to dive down into crime segments, using some of the multi-variate visualization techniques we covered in the later lessons of Data Analysis in R

To understand how I could best group crime report incidents by description, I surveyed all the potential values appearing in the columns `CrimeCat` and `Desc`.

I assessed `unique(crimes$Desc)` but suppressed the output for this column because there are about 1800 unique descriptions in the fields. This would not be a useful coulmn to use in grouping for plots.

On the other hand, the `CrimeCat` column includes about 60 unique categories as verified by `unique(crimes$CrimeCat)` This number is too large for tractable visualizations, but I decided I could group these into a smaller number of main categories (variable - `mainCat`) using grepl string matching.

Thus I grouped crimes by homicide, robbery/larceny, assault, rape, weapons, domestic violence, traffic and court viaolations and 'quality of life' (this was a class of crimes noted in the dataset, which included somewhat minor violations including 'curfew-loitering', drug possession and incidents related to public liquor consumption).

```{r Exploration - crime incidents per day grouped by crime type (a)}

crimes <- transform(crimes, mainCat = 
          ifelse(grepl("HOMICIDE", CrimeCat, ignore.case = T),"homicide",
          ifelse(grepl("ROBBERY|LARCENY", CrimeCat, ignore.case = T),"robbery",
          ifelse(grepl("ASSAULT", CrimeCat, ignore.case = T),"assault",
          ifelse(grepl("WEAPONS", CrimeCat, ignore.case = T),"weapons",
          ifelse(grepl("DOM-VIOL", CrimeCat, ignore.case = T),"domestic_violence",
          ifelse(grepl("RAPE", CrimeCat, ignore.case = T),"rape",
          ifelse(grepl("TRAFFIC", CrimeCat, ignore.case = T),"traffic",
          ifelse(grepl("COURT", CrimeCat, ignore.case = T),"court",       
                        ifelse(grepl("QUALITY|VANDALISM", CrimeCat, ignore.case = T),"quality_of_life","other"))))))))))

```

I then surveyed examples from the output with `head(crimes,50)` and `tail(crimes,50)` to ensure the new column assignment worked correctly.

As in previous examples, I created a new dataframe with dplyr functions, this time grouping by both date and the new `mainCat` variable.

Also - to note - I was seeing several report data points for dates in the future. I assume these were the result of data entry errors. For all plots going forward, I have chosen to subset by incidents occurring prior to a 'cut-off' date of June 2015.

```{r Exploration - line chart of crime incidents per day grouped by crime type (b)}

cutOffDate = '2015-06-01'
crime_types_by_day <- subset(crimes, date_format < cutOffDate) %>%
  group_by(date_format, mainCat) %>%
  summarize(incidents = sum(any_crime),
            n = n()) %>%
            ungroup() %>%
  arrange(date_format,incidents)

ggplot(aes(x= date_format, y=incidents), data=crime_types_by_day) +
  geom_line(aes(color = mainCat), alpha = 0.3) +
  geom_smooth(aes(color = mainCat))

```

The results are fairly messy with daily measures, so I decided to create a similar line plot with mainCat groupings but monthly crime incidents on the x-axis. For this I built a new `crime_types_by_month` dataframe, as seen below.

```{r Exploration - line plot of crime incidents per month grouped by crime type (a)}

crime_types_by_month <- subset(crimes, date_format < cutOffDate) %>%
  group_by(month, mainCat) %>%
  summarize(incidents = sum(any_crime),
            n = n()) %>%
            ungroup() %>%
  arrange(month,incidents)

ggplot(aes(x= month, y=incidents), data=crime_types_by_month) +
  geom_line(aes(color = mainCat), alpha = 0.8) +
  geom_smooth(aes(color = mainCat))

```

Interesting trends are visible when grouping incidents by type in a single plot, but the output is still fairly messy and dynamics for some categories are hard to discern, as the scales of total incidents in each crime category are substantially different.

Thus, I decided to re-plot `crime_types_by_month` in a facet wrap with 'free_y' scale to better view dynamics by crime type.

```{r Exploration - line chart of crime incidents per day and month grouped by crime type (b)}

ggplot(aes(x= month, y=incidents), data=crime_types_by_month) +
  geom_line(aes(color = mainCat)) +
  facet_wrap(~ mainCat, scales='free_y') +
  geom_smooth()

```

I found this chart to be striking - as incidents for all the main crime categories appear to have dropped significantly, while each shows a different pattern of decline. Some categries like Assault and Robbery plummeted around 2010 then remained steady, with others - like Homicide, Traffic, Domestic Violence and Other showing big drops later, around 2013 and 2014. 

The strange discontinuity at the 2014 year mark I noted earlier is also present in these (the later declining) categories, with incident counts holding steady in 2014 and 2015 after falling massively from much higher levels immediately before 2013 year-end.
  
### Initial Exploration: Rough plots with incidents segmented by location

Besides tracking crime dynamics by type, I also wanted to explore how crimes were distributed geographically by Police Beats. To do so, I used the Beat variable, which indicates where the crime occurred/was recorded. More detail on the 'Beat' variable is available in the Dataset description below and at [this link](http://rikiwiki.electronicartifacts.com/opd-crime-statistics/crimecatoak). There are 135 distinct police beats included as `Beat` values, with some values strictly numeric (like '31') and others having alphanumeric identifiers (like '26X').

Below is a map of police beats, as provided by the Oakland Police Department ([link here](http://oakgis.maps.arcgis.com/apps/OnePane/basicviewer/index.html?appid=69801e23ca8e456fbe6af18d2e88c614))

![Oakland Police Beat Map](https://raw.githubusercontent.com/burnssa/nd_project_3/master/Final%20Project/OaklandPoliceBeats.png)

Similar to the previous exploration, I built a new dataframe, grouping on incidents per month and `Beat`.

```{r Exploration - crime incidence by geography over time (a)}

crime_by_month_and_beat <- subset(crimes, date_format < cutOffDate) %>%
  group_by(month, Beat) %>%
  summarize(incidents = sum(any_crime),
            n = n()) %>%
            ungroup() %>%
  arrange(month)

```

First I plotted month and beat dynamics on one chart, associating each beat with a color. As seen, result was far too crowded to be useful. I thought perhaps a stacked bar chart of incidents by beat might show more insight - also plotted below. Clear insight wasn't visible in the column plots either.

```{r Exploration - crime incidence by geography over time (b)}

ggplot(aes(x= month, y=incidents), data=crime_by_month_and_beat) +
  geom_line(aes(color = Beat), alpha = 0.6) +
  geom_smooth(aes(color = Beat))

ggplot(crime_by_month_and_beat, aes(x = month, fill=Beat)) +
  geom_bar(bin = 30)

```

Going forward I decided that there are too many police beats to effectively display in a chart, and wanted to take the top 20 beats, then group incidents in other beats all under 'Other'. As you'll see in calculations below - these top 20 beats cover about 52% of all crime reports with descriptions (our `any_crime` variable).

First I created dataframe with total crime incidents regardless of date.

Then I added another column to the crimes - `mainBeat`, perserving the value for the top 20 beats by incidents, and labeling 'other' for all other beats.

Following the transformation, I revised the `crime_by_month_and_beat` dataframe with the mainBeat variable to produce clearer output, re-plotting with facet wrap to more clearly see the crime incident dynamic by beat.

```{r Exploration - crime incidence by geography over time}

total_crime_by_beat <- subset(crimes, date_format < cutOffDate) %>%
  group_by(Beat) %>%
  summarize(incidents = sum(any_crime),
            n = n()) %>%
  arrange(desc(incidents))

sum(total_crime_by_beat$incidents[1:20]) / sum(total_crime_by_beat$incidents)

topBeats = total_crime_by_beat$Beat[1:20]
crimes <- transform(crimes, mainBeat = ifelse((Beat %in% topBeats), as.character(Beat), 'other'))

crime_by_month_and_beat <- subset(crimes, date_format < cutOffDate) %>%
  group_by(month, mainBeat) %>%
  summarize(incidents = sum(any_crime),
            n = n()) %>%
            ungroup() %>%
  arrange(incidents)

ggplot(aes(x= month, y=incidents), data=crime_by_month_and_beat) +
  geom_line(aes(color = mainBeat)) +
  facet_wrap(~ mainBeat, scales='free_y') +
  geom_smooth()
summary(crime_by_month_and_beat)
```

Again, in the per-Beat facet breakdown we see uniformly down-trending crime incidence over time, with some beats experiencing more pronounced local spikes up around mid- to end of 2013. For some beats, including 04X, 08X, 20X and others, we also see the strange discontinuity at the end of 2013 that appeared in other views of monthly crime over time. 

For these police beats, crime stays steady or spikes toward the end of 2013, then precipitously drops right at the new year, and remains or declines from the lower level to the present day.

***
  
# Final Plots and Summary
For my final overview plots I chose to sharpen and adjust a few views we looked at in the Initial Exploration section, and to refine the associated fitted curves with linear models for each plot, instead of using the standard non-parametric geom_smooth function. 

### Final Plot 1: Aggregate view of weekly crime incidence in dataset
For the aggregate crime trends chart, I decided to use weekly crime incident data on my x-axis, as this could provide a balance between the bias and variance poles of daily and monthly periods from earlier plots. I also plotted using the log of y, and plotted with a linear model, looking run a similar regression to arrive at usefully interpretable coeffficients.

First, weekly analysis requires weekly units, and I cut `date_format` accordingly.

```{r Final Plot 1a - Aggregate Trends in Crime Rates, fig.width=20, fig.height=16 }

crimes$week <- as.Date(cut(crimes$date_format,
  breaks = "week"))

#Then: building dataframe with weekly crimve values
crime_by_week <- subset(crimes, date_format < cutOffDate) %>%
  group_by(week) %>%
  summarize(incidents = sum(any_crime),
            n = n()) %>%
  arrange(week)

ggplot(aes(x=week, y=log(incidents)), data=crime_by_week) +
  geom_line(color = 'darkblue', alpha = 0.8) +
  ylab("Log of Crimes Reported Per Week") +
  xlab("Date") +
  scale_y_continuous(breaks = seq(6,8,0.25)) +
  stat_smooth(method = 'lm', formula = I(y)~I(as.numeric(x)),  color = 'red') +
  ggtitle("Weekly Crime Incidence in Oakland:\n January 2007 to June 2015") +
  theme(plot.title = element_text(lineheight=.8, face="bold", size = 30)) +
  theme(axis.title.y = element_text(lineheight=1, face="bold", size=20, vjust=1.5)) +
  theme(axis.title.x = element_text(lineheight=1, face="bold", size=20, vjust=0.5))

```

For my final aggregate chart, I also wanted to set up a simple model for assessing crime trends. Looking at our trend line, I note that a linear model provides a very strong fit, with a F-test p-value of close to zero and a R-squared of around 0.82.

Based on the coefficient estimated, it looks like crime has been trending down at a rate of 11% per year on average.

```{r Final Plot 1b - Aggregate Trends in Crime Rates}

c1 <- lm(I(log(incidents)) ~ I(as.numeric(week)), data = crime_by_week)
summary(c1)

```

### Final Plot 2: View of monthly crime, faceted by crime type

In my second chart, I wanted to refine the faceted-by-crime-type charts I produced in the exploratory analysis, but use corresponding regression lines based on linear models for each facet as in Final Plot 1. For the facets, I decided to draw on monthly crime statistics, as weekly figures in certain categories were very sparse with too much variance, and I left the y-scale free.

```{r Final Plot 2a - Trends in Crime Rates by Crime Type, fig.width=24, fig.height=14}

ggplot(aes(x=month, y=log(incidents)), data=crime_types_by_month) +
  geom_line(aes(color = mainCat), alpha = 0.9) +
  ylab("Log of Crimes Reported Per Month") +
  xlab("Date") +
  facet_wrap(~ mainCat, scales='free_y', ncol=5) +
  stat_smooth(method = 'lm', formula = I(y)~I(as.numeric(x)),  color = 'red') +
  ggtitle("Monthly Crime Incidence in Oakland by Crime Category:\n January 2007 to June 2015") +
  theme(plot.title = element_text(lineheight=1, face="bold", size=30, vjust=1)) +
  theme(axis.title.y = element_text(lineheight=1, face="bold", size=20, vjust=1.5)) +
  theme(axis.title.x = element_text(lineheight=1, face="bold", size=20, vjust=0.5)) +
  scale_color_discrete(name = "Crime Category") +
  theme(strip.text.x = element_text(size = 18, colour = "blue", face='bold'))

```

Note that the discontinuities in crime reduction around the beginning of 2014 are even more striking when plotting the log of incidents on the y axis.

To test how the inclusion of crime type variables could improve the fit of our trend line, I decided to also include `mainCat` as a independent variable in a linear regression model. The addition seems to improve fit slightly versus a model of incidents vs time. R-squared for the new model is 0.85.

```{r Final Plot 2b - Trends in Crime Rates by Crime Type }

cm1 <- lm(I(log(incidents)) ~ I(as.numeric(month)), data = crime_types_by_month)
cm2 <- update(cm1, ~ . + mainCat)
summary(cm2)

```

### Final Plot 3: View of monthly crime, faceted by police beat

Similarly, I revisited my `crime_by_month_and_beat` to polish visuals showing how crime has trended by police beat. I used facet wrap to highlight the trend for each beat, and a log y scale as in Final Plots 1 and 2.

```{r Final Plot 3a - Trends in Crime Rates by crime type, fig.width=18, fig.height=24}

ggplot(aes(x=month, y=log(incidents)), data=crime_by_month_and_beat) +
  geom_line(aes(color = mainBeat), alpha = 0.9) +
  ylab("Log of Crimes Reported Per Month") +
  xlab("Date") +
  facet_wrap(~mainBeat, scales='free_y', ncol=3) +
  stat_smooth(method = 'lm', formula = I(y)~I(as.numeric(x)),  color = 'red') +
  ggtitle("Monthly Crime Incidence in Oakland by Police Beat:\n January 2007 to June 2015") +
  theme(plot.title = element_text(lineheight=1, face="bold", size = 30, vjust = 1.5)) +
  theme(axis.title.y = element_text(lineheight=1, face="bold", size=20, vjust=1.5)) +
  theme(axis.title.x = element_text(lineheight=1, face="bold", size=20, vjust=0.5)) +
  scale_color_discrete(name = "Police Beat") +
  theme(strip.text.x = element_text(size = 18, colour = "blue", face='bold'))

```

Mapping trends to geography, we can see where each of the top 20 police beats are located in our map below.

![Oakland Police Beat Map](https://raw.githubusercontent.com/burnssa/nd_project_3/master/Final%20Project/OaklandPoliceBeats.png)
Courtesy of the [Oakland Police Department](http://oakgis.maps.arcgis.com/apps/OnePane/basicviewer/index.html?appid=69801e23ca8e456fbe6af18d2e88c614)

Note that many of the top beats by crime incidence correspond to the 'hotter' regions in downtown Oakland as shown in our earlier exploratory heatmap, particularly '08X', '04X' and '06X'.

As a reminder, we can see a modified heatmap below, plotted over a Google roadmap.

```{r Final Plot 3c - crime heatmap reference, fig.width=20, fig.height=16}

oakland_roadmap_location <- c(-122.326, 37.74, -122.17, 37.85)
oakland_roadmap <- get_map(location = oakland_roadmap_location, source="google", maptype="roadmap")

ggmap(oakland_roadmap) +
  geom_density2d(data = crimesMapSubset, aes(x = Lng, y = Lat), size = .3) +
  stat_density2d(data = crimesMapSubset, aes(x = Lng, y = Lat, fill = ..level.., alpha = ..level..), size = 0.2, bins = 20, geom = 'polygon') +
  scale_fill_gradient(low = "blue", high = "red") +
  labs(x = 'Longitude', y = 'Latitude') + 
  ggtitle('Crime Heatmap for Oakland, CA:\n January 2007 to June 2015') +
  theme(plot.title = element_text(lineheight=1, face="bold", size = 30, vjust = 1.5)) +
  theme(axis.title.y = element_text(lineheight=1, size=14, vjust=1.5)) +
  theme(axis.title.x = element_text(lineheight=1, size=14, vjust=0.5))
  
```

I chose to close out the exploratory analysis with a comparison of crime heatmaps for periods at the beginning and end of our dataset. Two heatmaps are arranged with a similar density scale to see what the distribution of crime looked like in Oakland during 2008 vs 2014. Consistent with earlier analysis, by 2014 the map is 'cooler' overall, with bright hotspots more subdued.

```{r Final Plot 3d - crime heatmap reference - 2014 vs 2008, fig.width=30, fig.height=24}

crimes_two_o_eight <- subset(crimesMapSubset, date_format >= '2008-01-01' & date_format < '2009-01-01' )
crimes_two_fourteen <- subset(crimesMapSubset, date_format >= '2014-01-01' & date_format < '2015-01-01' )

two_o_eight_map <- ggmap(oakland_roadmap) +
        stat_density2d(data = crimes_two_o_eight, aes(x = Lng, y = Lat, fill = ..level.., alpha = ..level..), size = 0.2, bins = 20, geom = 'polygon', limits=c(0,1000),breaks=seq(0,1000,by=100)) +
        scale_fill_gradient(low = "blue", high = "red", limits=c(0,1000)) +
        labs(x = 'Longitude', y = 'Latitude') + 
        ggtitle('Crime Heatmap for Oakland, CA:\n January 2008 to December 2008') +
        theme(plot.title = element_text(lineheight=1, face="bold", size = 30, vjust = 1.5)) +
        theme(axis.title.y = element_text(lineheight=1, size=20, vjust=1.5)) +
        theme(axis.title.x = element_text(lineheight=1, size=20, vjust=0.5)) +
        scale_alpha_continuous(limits=c(0,1000))

two_fourteen_map <- ggmap(oakland_roadmap) +
        stat_density2d(data = crimes_two_fourteen, aes(x = Lng, y = Lat, fill = ..level.., alpha = ..level..), size = 0.2, bins = 20, geom = 'polygon', limits=c(0,1000),breaks=seq(0,1000,by=100)) +
        scale_fill_gradient(low = "blue", high = "red", limits=c(0,1000)) +
        labs(x = 'Longitude', y = 'Latitude') + 
        ggtitle('Crime Heatmap for Oakland, CA:\n January 2014 to December 2014') +
        theme(plot.title = element_text(lineheight=1, face="bold", size = 30, vjust = 1.5)) +
        theme(axis.title.y = element_text(lineheight=1, size=20, vjust=1.5)) +
        theme(axis.title.x = element_text(lineheight=1, size=20, vjust=0.5)) +
        scale_alpha_continuous(limits=c(0,1000))

grid.arrange(two_o_eight_map, two_fourteen_map, ncol=2)
  
```

To test how police beat data could improve our trend fit I decided to also include `mainBeat` as a independent variable in a linear model with monthly data. The addition seems to improve fit slightly - the R-squared when modeling with police beat values reaches 0.93.

```{r Final Plot 3b - Trends in Crime Rates by crime type}

cmb1 <- lm(I(log(incidents)) ~ I(as.numeric(month)), data = crime_by_month_and_beat)
cmb2 <- update(cmb1, ~ . + mainBeat)
summary(cmb2)

```

***

# Reflection

My biggest struggles with the dataset related to the process of grouping data into dataframes with incidents over useful time periods, transforming data to proper formats, and segmenting by tractable crime and police beat groups. Going through the steps required for my initial and final plots, I learned much about string matching, dataframe reshaping and subsetting in R, as well as heatmap plotting with ggmap.

Once the data were properly grouped and I could run plots, I found the output to be fascinating. Coming across the dataset was a huge success for me, as it is a rich source of insight on a topic in which I have a strong interest. Approaches to plotting learned in the Data Analysis in R course proved useful for visualizing elements of the dataset to reveal the evidence highlighted above.

The exploratory analysis here prompts me to explore a few more questions on the crime data. A few that are top-of-mind I outline below:

1. What caused the massive, sudden drop at the beginning of 2014 in reported crimes for many crime categories? The discontinuity at the change in the year is so striking, it seems that it must be due to a change in policing or reporting policy, rather than a strange and dramatic drop-off in crimes committed.
2. How are crime types and police beats correlated? Are there specific regions associated with particular crimes?
3. What does the heatmap look like for specific crime times (e.g. violent crime or robbery)?
4. With additional data, I'd love to explore how changes in crime rates have been associated with changes in economic outcomes, property values and educational indicators in Oakland. 

In my explorations, I also came across many references to packages that might be useful for time-series analysis in the future, such as ['zoo'](https://cran.r-project.org/web/packages/zoo/index.html). I'd like to try these out on other datasets.

***

# Refences and Sources
Melt: http://www.r-bloggers.com/melt/ \n

Reshape background: http://seananderson.ca/2013/10/19/reshape.html \n

Melting for time series: http://stackoverflow.com/questions/1181060/reshaping-time-series-data-from-wide-to-tall-format-for-plotting \n

Reshaping data: http://www.r-bloggers.com/reshape-and-aggregate-data-with-the-r-package-reshape2/

Creating columns with if-else statements: http://stackoverflow.com/questions/13672781/populate-a-column-using-if-statements-in-r \n

String matching with grepl: http://www.endmemo.com/program/R/grepl.php

Choosing between regression models: http://stats.stackexchange.com/questions/43930/choosing-between-lm-and-glm-for-a-log-transformed-response-variable \n

Creating graph titles with ggplot: 
http://www.cookbook-r.com/Graphs/Titles_(ggplot2)/ \n
http://zevross.com/blog/2014/08/04/beautiful-plotting-in-r-a-ggplot2-cheatsheet-3/

Overlaying fitted regressions in ggplot: http://stackoverflow.com/questions/1476185/how-to-overlay-a-line-for-an-lm-object-on-a-ggplot2-scatterplot \n
http://stackoverflow.com/questions/10528631/add-exp-power-trend-line-to-a-ggplot \n

Working with ggmap:
https://www.nceas.ucsb.edu/~frazier/RSpatialGuides/ggmap/ggmapCheatsheet.pdf
https://cran.r-project.org/web/packages/ggmap/ggmap.pdf
http://www.geo.ut.ee/aasa/LOOM02331/heatmap_in_R.html


***

# Dataset Description 

### Crime reports from Oakland: January 2007 to July 2015

###Description
A dataset containing discriptions, event timing, and geographic information from over 690,000 crime reports in Oakland during the period 2007 to mid-2015

###Usage
Read in the dataset from csv

###Format
A data frame with 696372 rows and 21 variables

###Details
See more details on the dataset in a wiki page built by [Rik Belew](http://rikiwiki.electronicartifacts.com/about-me), at this [link](http://rikiwiki.electronicartifacts.com/opd-crime-statistics/crimecatoak), with an associated Data Dictionary [here](http://rikiwiki.electronicartifacts.com/opd-crime-statistics/combined-opd_usc-dataset).

  
- Idx. arbitrary unique identifier (0 - 696,372)
- OPD_RD. the crime identifier originally assigned by OPD
- OIdx. OIdx: This field ranges from zero up to the number of individual USC (see below) records associated with a particular incident. Recall, while OPD only reveals a single record associated with each crime incident, multiple crime records are maintained internally.  If one or more USC (see below) records are available, they are indexed with numbers 1, 2, 3 etc; when there are none, OIdx=0.
- Date. This is the date associated with the crime by OPD
- Time. This is the time associated with the crime by OPD
- CType. This is the text string crime description provided by OPD
- Desc. This is a more detailed text string crime provided by OPD
- Beat. The police beat associated with the crime by OPD (135 unique records - see below for beat map)
- Addr. Geographic location of incident - see geographic note below
- Lat. Geographic location of incident - see geographic note below
- Lng. Geographic location of incident - see geographic note below
- Src. Source of data
- UCR. Indicator used by USC* in compiling crime data
- Statute. Legal statute violation associated with crime

Geographic note:
The original source of these variables was the OPD record. However, because addresses act as critique link to geocoding (latitude longitude coordinates), special procedures were used to normalize and “cache” address strings used more than once.  This allows the efficiency of minimizing the number of required geocoding queries. A by product of this process is that more complete, normalized addresses generated in this case. In particular, these generally include a zip code. Note this allows “extrapolation” from crimes for which addresses were geocoded, to provide geocodes for other crimes sharing the same address.

*USC: Urban Strategies Council - organization contributing to the dataset

Courtesy of the [Oakland Police Department](http://oakgis.maps.arcgis.com/apps/OnePane/basicviewer/index.html?appid=69801e23ca8e456fbe6af18d2e88c614)

***