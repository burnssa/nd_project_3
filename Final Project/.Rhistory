crimesMapSubset <- subset(crimes, (Lat > 37.7 & Lat < 37.9) &
(Lng > -122.35 & Lng < -122.15))
```
Crime seems to skew toward the mid-Northern and Western parts of the city, based on the histogram.
For a more detailed view of geographic distribution, I decided to use ggmap to plot a basic heatmap of crime in Oakland for the period in question.
```{r Exploration - crime heatmap by longitude and latitude}
oakland_location <- c(-122.33, 37.71, -122.15, 37.9)
oakland_map <- get_map(location = oakland_location, source="google",
maptype="hybrid")
ggmap(oakland_map) +
geom_density2d(data = crimesMapSubset, aes(x = Lng, y = Lat), size = 1) +
stat_density2d(data = crimesMapSubset, aes(x = Lng, y = Lat,
fill = ..level.., alpha = ..level..),
size = 0.01, bins = 20, geom = 'polygon') +
scale_fill_gradient(low = "green", high = "red") +
guides(fill = guide_legend(position='None'))
```
In the heatmap, we see a strong concentration of crimes in the downtown area, with a pocket also slightly to the northwest along San Pablo Avenue, then all along International Avenue to the southeast.
For another view of Oakland crime dynamics, I created a line plot of incidents per day, using the dplyr aggregation techniques we learned in the Data Analysis in R course.
_Note: creating new dataframe based on incidents per day, using dplyr 'verbose' method, then using this in line plot._
```{r Exploration - line plot of crime incidents per day over time}
crime_day <- group_by(crimes, date_format)
crime_by_day <- summarize(crime_day,
incidents = sum(any_crime),
n=n())
ggplot(aes(x=date_format, y=incidents), data=crime_by_day) +
geom_line() +
geom_smooth()
```
In the daily line plot, the decline of crime reports over time is clearly visible.
As daily crime incident counts are fairly noisy, I wanted to take a look at potentially smoother time increments - creating a line plot of `any_crime` incidents each month.
To create the monthly plot, I cut our `date_format` data in monthly units, using the syntax we learned in lesson 5 of the Data Analysis in R course, then aggregated crime data by month. To do so, I applied dplyr methods again, but this time using the 'concise' syntax.
```{r Exploration - line plot of crime incidents per month over time}
crimes$month <- as.Date(cut(crimes$date_format,
breaks = "month"))
crime_by_month <- crimes %>%
group_by(month) %>%
summarize(incidents = sum(any_crime),
n = n()) %>%
arrange(month)
ggplot(aes(x=month, y=incidents), data=crime_by_month) +
geom_line() +
geom_smooth(aes(group = 1))
```
The downward trend in crime reports is again evident in plotted monthly crime report data. Another striking feature of the chart is the large vertical drop in incident count at the beginning of 2014.
It seems very strange to me that the reported number of crimes would be fairly constant throughout 2012 and 2013, fall by around 40% as the year changed, then persist at a largely constant lower level around 3,700 incidents per month for the next year. I wonder if there was a major change in the way crimes were recorded or reported starting at the beginning of 2014.
To better understand the distribution of daily crime rates, I also plotted mean, median and quartile measures of daily crime rates.
Also - to note - I was seeing several report data points for dates in the future. I assume these were the result of data entry errors. For all plots going forward, I have chosen to subset by incidents occurring prior to a 'cut-off' date of June 2015.
*Mean and median daily crime rates:*
```{r Exploration - line plot of mean and median daily crime rates per month over time (a)}
cutOffDate = '2015-06-01'
crime_by_day <- subset(crime_by_day, date_format < cutOffDate)
crime_by_day$month <- as.Date(cut(crime_by_day$date_format,
breaks = "month"))
daily_by_month <- crime_by_day %>%
group_by(month) %>%
summarize(mean_incidents = mean(incidents),
median_incidents = median(incidents),
bottom_quartile = quantile(incidents, 0.25),
top_quartile = quantile(incidents, 0.75),
bottom_decile = quantile(incidents, 0.1),
top_decile = quantile(incidents, 0.9),
n = n()) %>%
arrange(month)
ggplot(daily_by_month, aes(x=month, y=mean_incidents)) +
geom_line(color = 'blue') +
geom_line(data = daily_by_month, aes(x=month, y=median_incidents, color = 'red')) +
theme(legend.position = "none")
```
### Initial Exploration: Rough plots with incidents segmented by crime type
After having uncovered some interesting insights about local crime trends at an aggregate level, I wanted to dive down into crime segments, using some of the multi-variate visualization techniques we covered in the later lessons of Data Analysis in R
To understand how I could best group crime report incidents by description, I surveyed all the potential values appearing in the columns `CrimeCat` and `Desc`.
I assessed `unique(crimes$Desc)` but suppressed the output for this column because there are about 1800 unique descriptions in the fields. This would not be a useful coulmn to use in grouping for plots.
On the other hand, the `CrimeCat` column includes about 60 unique categories as verified by `unique(crimes$CrimeCat)` This number is too large for tractable visualizations, but I decided I could group these into a smaller number of main categories (variable - `mainCat`) using grepl string matching.
Thus I grouped crimes by homicide, robbery/larceny, assault, rape, weapons, domestic violence, traffic and court viaolations and 'quality of life' (this was a class of crimes noted in the dataset, which included somewhat minor violations including 'curfew-loitering', drug possession and incidents related to public liquor consumption).
```{r Exploration - crime incidents per day grouped by crime type (a)}
crimes <- transform(crimes, mainCat =
ifelse(grepl("HOMICIDE", CrimeCat, ignore.case = T),"homicide",
ifelse(grepl("ROBBERY|LARCENY", CrimeCat, ignore.case = T),"robbery",
ifelse(grepl("ASSAULT", CrimeCat, ignore.case = T),"assault",
ifelse(grepl("WEAPONS", CrimeCat, ignore.case = T),"weapons",
ifelse(grepl("DOM-VIOL", CrimeCat, ignore.case = T),
"domestic_violence",
ifelse(grepl("RAPE", CrimeCat, ignore.case = T),"rape",
ifelse(grepl("TRAFFIC", CrimeCat, ignore.case = T),"traffic",
ifelse(grepl("COURT", CrimeCat, ignore.case = T),"court",
ifelse(grepl("QUALITY|VANDALISM", CrimeCat, ignore.case = T),
"quality_of_life","other"))))))))))
trimmed_crimes_df <- crimes[,c("Lat","Lng","mainCat","date_format", "time_format")]
write.csv(crimes, file = "oakland_crimes.csv")
```{r setup, cache=FALSE, include=FALSE, message=FALSE}
library(knitr)
output <- opts_knit$get("rmarkdown.pandoc.to")
if (output=="html") opts_chunk$set(fig.width=6, fig.height=6, warning=FALSE, message=FALSE, tidy=TRUE, echo=FALSE)
```
=======================================================
Scott Burns
Udacity â€“ Data Analyst Nanodegree
Project 3: Data Analysis with R
========================================================
# Introduction and Background
###Why I chose this dataset
I live in the city of Oakland, California, and plan to stay here. I'm curious about crime trends in the city, as they may have a direct impact on my life and important decisions I encounter in the future. Thus as I was exploring dataset options for this project, I was excited to uncover [this dataset](http://data.openoakland.org/dataset/crime-reports/resource/49bee847-a9b7-4e71-84d8-3f4cabb26cf0) on the website OpenOakland.org - which has records of all crime reports from the city of Oakland from 2007 to earlier this summer, including incident details such as geographic location and crime type. It looked fascinating to me, and I decided to dive in.
###Questions initially considered
1. How has the incidence of crime been trending in the city over the last 8 years?
2. Have incidences of particular types of crime been growing / diminishing differentially?
3. Where is crime occurring geographically?
4. How has crime incidence changed in particular areas within the city?
Primarily, my focus was on generating a few thought-provoking visualizations, to see if they might suggest directions for more in-depth exploration.
###About the dataset
The data were downloaded from [data.openoakland.org](http://data.openoakland.org/dataset/crime-reports/resource/49bee847-a9b7-4e71-84d8-3f4cabb26cf0).
Additional background on the dataset is available [on Rik Belew's blog ](http://rikiwiki.electronicartifacts.com/wp-content/uploads/2013/05/showCrime_v21.pdf).
Background on the dataset's CrimeCat classifications can be found [on this explainer page](http://rikiwiki.electronicartifacts.com/opd-crime-statistics/crimecatoak).
More detailed background on the dataset is available in Dataset Description section at the end of this file.
***
#Stream-of-Consciousness Exploration
### Getting Started: Preparing the RStudio environment
In preparation for analysis, I loaded in the dataset of interest, and glanced at summary information about it (output suppressed).
```{r To get started - reading in dataset and preparing libraries, include=FALSE}
setwd('/Users/burnssa/Google Drive/Personal Files/Education/Data Science Nanodegree/Project 3/Final Project')
getwd()
crimes <- read.csv('OPD_150708.csv')
summary(crimes) #Output suppressed for final report
head(crimes) #to illustrate the available fields, with example values
#Include libraries - all critical to the analysis
library(dplyr)
library(ggplot2)
#install.packages('ggmap')
library(ggmap)
library("reshape2")
library(RColorBrewer)
#install.packages('memisc')
library(gridExtra)
library(memisc)
library(scales)
```
I also created two variables to potentially use throughout subsequent plots.
1. An `any_crime` dummy variable. For this, any record where `Desc` and `CrimeCat` are not blank is assigned a 1 value. If a record doesn't even have this minimal information about the crime, I think it's better to exclude it as an incident.
```{r}
crimes$any_crime <- with(crimes, (Desc != "" & CrimeCat != "")) * 1
```
2. Add column `date_format` with date representation of `Date` string
```{r}
crimes$date_format <- as.Date(crimes$Date, format = "%m/%d/%y")
```
### Initial Exploration: Rough plots of crime data
As a first basic attempt to visualize the data, I plotted a histogram of crime reports, using a binwidth of 30 days.
```{r Exploration - aggregate trend in daily crime over full dataset period with date histogram}
ggplot(crimes, aes(x = date_format)) +
geom_histogram(binwidth = 30)
```
From the histogram, we see indications that crime reports have fallen significantly in Oakland over the last 7 years, as report counts per 30-day period in 2007 and 2008 appear to number between 9,000 and 10,000, while counts per period have been around 4,000 in recent years.
I was also curious how crimes were distributed throughout the day over the period in question. To see the dynamics, I plotted another histogram by time of day, with hourly bins, creating an `hour` variable to more easily bin incidents.
```{r Exploration - crime incidence by time of day (a)}
crimes$time_format <- as.POSIXct(strptime(crimes$Time, "%H:%M:%S"))
#as.POSIXct added to address errors grouping in dplyr
crimes$hour <- as.numeric(format(crimes$time_format, "%H"))
ggplot(crimes, aes(x = hour)) +
geom_histogram(binwidth = 1)
```
The hourly distribution suprised me - first by the huge spike at hour 0 (e.g. midnight to 1am). I suspected that this might be related to data entry issues, so I also checked the composition of crimes that were committed at hour zero, loading them from the original `Time` variable. From this, I discovered that 98,823 of the 118,590 `one_am` records have the value exactly "0:00:00". This strengthened my suspicion about the spike being driven by coding issues.
Relatively confident that most of the 98,823 records occurring at midnight were probably set in the absence of a known time for a crime, or as a result of time recording errors, I decided to run the histogram on a subset that removed the 'exactly midnight' crimes.
```{r Exploration - crime incidence by time of day (b)}
one_am <- subset(crimes, substr(as.character(Time),1,1) == "0" )
summary(one_am$Time)
ggplot(subset(crimes, as.character(Time) != "0:00:00") , aes(x = hour)) +
geom_histogram(binwidth = 1)
```
In the modified histogram, I was also somewhat surprised that crimes don't spike to a greater extent in the evenings, with incident rates staying fairly uniform from 10am to 4pm, then rising to a moderate peak around 7pm and declining from there to a minimum at the 5am hour.
Next, I wanted to explore the distribution of crime by latitude and longitude, with histograms for each variable.
After a first glance at raw plots for each, I realized I needed to remove a few obviously incorrect entries, to arrive at the histograms below, and obtained more useful ranges for a chart by summarizing reasonable values for `Lat`.
```{r Exploration - histograms by longitude and latitude}
summary(subset(crimes$Lat, (crimes$Lat < 40 & crimes$Lat > 37)))
ggplot(subset(crimes, (Lat > 37.7 & Lat < 37.9)), aes(x = Lat)) +
geom_histogram()
summary(subset(crimes$Lng, (crimes$Lng > -123 & crimes$Lng < -122.1 )))
ggplot(subset(crimes, (Lng > -122.35 & Lng < -122.15)), aes(x = Lng)) +
geom_histogram()
crimesMapSubset <- subset(crimes, (Lat > 37.7 & Lat < 37.9) &
(Lng > -122.35 & Lng < -122.15))
```
Crime seems to skew toward the mid-Northern and Western parts of the city, based on the histogram.
For a more detailed view of geographic distribution, I decided to use ggmap to plot a basic heatmap of crime in Oakland for the period in question.
```{r Exploration - crime heatmap by longitude and latitude}
oakland_location <- c(-122.33, 37.71, -122.15, 37.9)
oakland_map <- get_map(location = oakland_location, source="google",
maptype="hybrid")
ggmap(oakland_map) +
geom_density2d(data = crimesMapSubset, aes(x = Lng, y = Lat), size = 1) +
stat_density2d(data = crimesMapSubset, aes(x = Lng, y = Lat,
fill = ..level.., alpha = ..level..),
size = 0.01, bins = 20, geom = 'polygon') +
scale_fill_gradient(low = "green", high = "red") +
guides(fill = guide_legend(position='None'))
```
In the heatmap, we see a strong concentration of crimes in the downtown area, with a pocket also slightly to the northwest along San Pablo Avenue, then all along International Avenue to the southeast.
For another view of Oakland crime dynamics, I created a line plot of incidents per day, using the dplyr aggregation techniques we learned in the Data Analysis in R course.
_Note: creating new dataframe based on incidents per day, using dplyr 'verbose' method, then using this in line plot._
```{r Exploration - line plot of crime incidents per day over time}
crime_day <- group_by(crimes, date_format)
crime_by_day <- summarize(crime_day,
incidents = sum(any_crime),
n=n())
ggplot(aes(x=date_format, y=incidents), data=crime_by_day) +
geom_line() +
geom_smooth()
```
In the daily line plot, the decline of crime reports over time is clearly visible.
As daily crime incident counts are fairly noisy, I wanted to take a look at potentially smoother time increments - creating a line plot of `any_crime` incidents each month.
To create the monthly plot, I cut our `date_format` data in monthly units, using the syntax we learned in lesson 5 of the Data Analysis in R course, then aggregated crime data by month. To do so, I applied dplyr methods again, but this time using the 'concise' syntax.
```{r Exploration - line plot of crime incidents per month over time}
crimes$month <- as.Date(cut(crimes$date_format,
breaks = "month"))
crime_by_month <- crimes %>%
group_by(month) %>%
summarize(incidents = sum(any_crime),
n = n()) %>%
arrange(month)
ggplot(aes(x=month, y=incidents), data=crime_by_month) +
geom_line() +
geom_smooth(aes(group = 1))
```
The downward trend in crime reports is again evident in plotted monthly crime report data. Another striking feature of the chart is the large vertical drop in incident count at the beginning of 2014.
It seems very strange to me that the reported number of crimes would be fairly constant throughout 2012 and 2013, fall by around 40% as the year changed, then persist at a largely constant lower level around 3,700 incidents per month for the next year. I wonder if there was a major change in the way crimes were recorded or reported starting at the beginning of 2014.
To better understand the distribution of daily crime rates, I also plotted mean, median and quartile measures of daily crime rates.
Also - to note - I was seeing several report data points for dates in the future. I assume these were the result of data entry errors. For all plots going forward, I have chosen to subset by incidents occurring prior to a 'cut-off' date of June 2015.
*Mean and median daily crime rates:*
```{r Exploration - line plot of mean and median daily crime rates per month over time (a)}
cutOffDate = '2015-06-01'
crime_by_day <- subset(crime_by_day, date_format < cutOffDate)
crime_by_day$month <- as.Date(cut(crime_by_day$date_format,
breaks = "month"))
daily_by_month <- crime_by_day %>%
group_by(month) %>%
summarize(mean_incidents = mean(incidents),
median_incidents = median(incidents),
bottom_quartile = quantile(incidents, 0.25),
top_quartile = quantile(incidents, 0.75),
bottom_decile = quantile(incidents, 0.1),
top_decile = quantile(incidents, 0.9),
n = n()) %>%
arrange(month)
ggplot(daily_by_month, aes(x=month, y=mean_incidents)) +
geom_line(color = 'blue') +
geom_line(data = daily_by_month, aes(x=month, y=median_incidents, color = 'red')) +
theme(legend.position = "none")
```
### Initial Exploration: Rough plots with incidents segmented by crime type
After having uncovered some interesting insights about local crime trends at an aggregate level, I wanted to dive down into crime segments, using some of the multi-variate visualization techniques we covered in the later lessons of Data Analysis in R
To understand how I could best group crime report incidents by description, I surveyed all the potential values appearing in the columns `CrimeCat` and `Desc`.
I assessed `unique(crimes$Desc)` but suppressed the output for this column because there are about 1800 unique descriptions in the fields. This would not be a useful coulmn to use in grouping for plots.
On the other hand, the `CrimeCat` column includes about 60 unique categories as verified by `unique(crimes$CrimeCat)` This number is too large for tractable visualizations, but I decided I could group these into a smaller number of main categories (variable - `mainCat`) using grepl string matching.
Thus I grouped crimes by homicide, robbery/larceny, assault, rape, weapons, domestic violence, traffic and court viaolations and 'quality of life' (this was a class of crimes noted in the dataset, which included somewhat minor violations including 'curfew-loitering', drug possession and incidents related to public liquor consumption).
```{r Exploration - crime incidents per day grouped by crime type (a)}
crimes <- transform(crimes, mainCat =
ifelse(grepl("HOMICIDE", CrimeCat, ignore.case = T),"homicide",
ifelse(grepl("ROBBERY|LARCENY", CrimeCat, ignore.case = T),"robbery",
ifelse(grepl("ASSAULT", CrimeCat, ignore.case = T),"assault",
ifelse(grepl("WEAPONS", CrimeCat, ignore.case = T),"weapons",
ifelse(grepl("DOM-VIOL", CrimeCat, ignore.case = T),
"domestic_violence",
ifelse(grepl("RAPE", CrimeCat, ignore.case = T),"rape",
ifelse(grepl("TRAFFIC", CrimeCat, ignore.case = T),"traffic",
ifelse(grepl("COURT", CrimeCat, ignore.case = T),"court",
ifelse(grepl("QUALITY|VANDALISM", CrimeCat, ignore.case = T),
"quality_of_life","other"))))))))))
trimmed_crimes_df <- crimes[,c("Lat","Lng","mainCat","date_format", "time_format")]
write.csv(trimmed_crimes_df, file = "oakland_crimes.csv")
```{r setup, cache=FALSE, include=FALSE, message=FALSE}
library(knitr)
output <- opts_knit$get("rmarkdown.pandoc.to")
if (output=="html") opts_chunk$set(fig.width=6, fig.height=6, warning=FALSE, message=FALSE, tidy=TRUE, echo=FALSE)
```
=======================================================
Scott Burns
Udacity â€“ Data Analyst Nanodegree
Project 3: Data Analysis with R
========================================================
# Introduction and Background
###Why I chose this dataset
I live in the city of Oakland, California, and plan to stay here. I'm curious about crime trends in the city, as they may have a direct impact on my life and important decisions I encounter in the future. Thus as I was exploring dataset options for this project, I was excited to uncover [this dataset](http://data.openoakland.org/dataset/crime-reports/resource/49bee847-a9b7-4e71-84d8-3f4cabb26cf0) on the website OpenOakland.org - which has records of all crime reports from the city of Oakland from 2007 to earlier this summer, including incident details such as geographic location and crime type. It looked fascinating to me, and I decided to dive in.
###Questions initially considered
1. How has the incidence of crime been trending in the city over the last 8 years?
2. Have incidences of particular types of crime been growing / diminishing differentially?
3. Where is crime occurring geographically?
4. How has crime incidence changed in particular areas within the city?
Primarily, my focus was on generating a few thought-provoking visualizations, to see if they might suggest directions for more in-depth exploration.
###About the dataset
The data were downloaded from [data.openoakland.org](http://data.openoakland.org/dataset/crime-reports/resource/49bee847-a9b7-4e71-84d8-3f4cabb26cf0).
Additional background on the dataset is available [on Rik Belew's blog ](http://rikiwiki.electronicartifacts.com/wp-content/uploads/2013/05/showCrime_v21.pdf).
Background on the dataset's CrimeCat classifications can be found [on this explainer page](http://rikiwiki.electronicartifacts.com/opd-crime-statistics/crimecatoak).
More detailed background on the dataset is available in Dataset Description section at the end of this file.
***
#Stream-of-Consciousness Exploration
### Getting Started: Preparing the RStudio environment
In preparation for analysis, I loaded in the dataset of interest, and glanced at summary information about it (output suppressed).
```{r To get started - reading in dataset and preparing libraries, include=FALSE}
setwd('/Users/burnssa/Google Drive/Personal Files/Education/Data Science Nanodegree/Project 3/Final Project')
getwd()
crimes <- read.csv('OPD_150708.csv')
```{r setup, cache=FALSE, include=FALSE, message=FALSE}
library(knitr)
output <- opts_knit$get("rmarkdown.pandoc.to")
if (output=="html") opts_chunk$set(fig.width=6, fig.height=6, warning=FALSE, message=FALSE, tidy=TRUE, echo=FALSE)
```
=======================================================
Scott Burns
Udacity â€“ Data Analyst Nanodegree
Project 3: Data Analysis with R
========================================================
# Introduction and Background
###Why I chose this dataset
I live in the city of Oakland, California, and plan to stay here. I'm curious about crime trends in the city, as they may have a direct impact on my life and important decisions I encounter in the future. Thus as I was exploring dataset options for this project, I was excited to uncover [this dataset](http://data.openoakland.org/dataset/crime-reports/resource/49bee847-a9b7-4e71-84d8-3f4cabb26cf0) on the website OpenOakland.org - which has records of all crime reports from the city of Oakland from 2007 to earlier this summer, including incident details such as geographic location and crime type. It looked fascinating to me, and I decided to dive in.
###Questions initially considered
1. How has the incidence of crime been trending in the city over the last 8 years?
2. Have incidences of particular types of crime been growing / diminishing differentially?
3. Where is crime occurring geographically?
4. How has crime incidence changed in particular areas within the city?
Primarily, my focus was on generating a few thought-provoking visualizations, to see if they might suggest directions for more in-depth exploration.
###About the dataset
The data were downloaded from [data.openoakland.org](http://data.openoakland.org/dataset/crime-reports/resource/49bee847-a9b7-4e71-84d8-3f4cabb26cf0).
Additional background on the dataset is available [on Rik Belew's blog ](http://rikiwiki.electronicartifacts.com/wp-content/uploads/2013/05/showCrime_v21.pdf).
Background on the dataset's CrimeCat classifications can be found [on this explainer page](http://rikiwiki.electronicartifacts.com/opd-crime-statistics/crimecatoak).
More detailed background on the dataset is available in Dataset Description section at the end of this file.
***
#Stream-of-Consciousness Exploration
### Getting Started: Preparing the RStudio environment
In preparation for analysis, I loaded in the dataset of interest, and glanced at summary information about it (output suppressed).
```{r To get started - reading in dataset and preparing libraries, include=FALSE}
setwd('/Users/burnssa/Google Drive/Personal Files/Education/Data Science Nanodegree/Project 3/Final Project')
getwd()
crimes <- read.csv('OPD_150708.csv')
```{r setup, cache=FALSE, include=FALSE, message=FALSE}
library(knitr)
output <- opts_knit$get("rmarkdown.pandoc.to")
if (output=="html") opts_chunk$set(fig.width=6, fig.height=6, warning=FALSE, message=FALSE, tidy=TRUE, echo=FALSE)
```
=======================================================
Scott Burns
Udacity â€“ Data Analyst Nanodegree
Project 3: Data Analysis with R
========================================================
# Introduction and Background
###Why I chose this dataset
I live in the city of Oakland, California, and plan to stay here. I'm curious about crime trends in the city, as they may have a direct impact on my life and important decisions I encounter in the future. Thus as I was exploring dataset options for this project, I was excited to uncover [this dataset](http://data.openoakland.org/dataset/crime-reports/resource/49bee847-a9b7-4e71-84d8-3f4cabb26cf0) on the website OpenOakland.org - which has records of all crime reports from the city of Oakland from 2007 to earlier this summer, including incident details such as geographic location and crime type. It looked fascinating to me, and I decided to dive in.
###Questions initially considered
1. How has the incidence of crime been trending in the city over the last 8 years?
2. Have incidences of particular types of crime been growing / diminishing differentially?
3. Where is crime occurring geographically?
4. How has crime incidence changed in particular areas within the city?
Primarily, my focus was on generating a few thought-provoking visualizations, to see if they might suggest directions for more in-depth exploration.
###About the dataset
The data were downloaded from [data.openoakland.org](http://data.openoakland.org/dataset/crime-reports/resource/49bee847-a9b7-4e71-84d8-3f4cabb26cf0).
Additional background on the dataset is available [on Rik Belew's blog ](http://rikiwiki.electronicartifacts.com/wp-content/uploads/2013/05/showCrime_v21.pdf).
Background on the dataset's CrimeCat classifications can be found [on this explainer page](http://rikiwiki.electronicartifacts.com/opd-crime-statistics/crimecatoak).
More detailed background on the dataset is available in Dataset Description section at the end of this file.
***
#Stream-of-Consciousness Exploration
### Getting Started: Preparing the RStudio environment
In preparation for analysis, I loaded in the dataset of interest, and glanced at summary information about it (output suppressed).
```{r To get started - reading in dataset and preparing libraries, include=FALSE}
setwd('/Users/burnssa/Google Drive/Personal Files/Education/Data Science Nanodegree/Project 3/Final Project')
getwd()
crimes <- read.csv('OPD_150708.csv')
summary(crimes) #Output suppressed for final report
head(crimes) #to illustrate the available fields, with example values
#Include libraries - all critical to the analysis
library(dplyr)
library(ggplot2)
#install.packages('ggmap')
library(ggmap)
library("reshape2")
library(RColorBrewer)
#install.packages('memisc')
library(gridExtra)
library(memisc)
library(scales)
```
I also created two variables to potentially use throughout subsequent plots.
1. An `any_crime` dummy variable. For this, any record where `Desc` and `CrimeCat` are not blank is assigned a 1 value. If a record doesn't even have this minimal information about the crime, I think it's better to exclude it as an incident.
```{r}
crimes$any_crime <- with(crimes, (Desc != "" & CrimeCat != "")) * 1
```
2. Add column `date_format` with date representation of `Date` string
```{r}
crimes$date_format <- as.Date(crimes$Date, format = "%m/%d/%y")
```
### Initial Exploration: Rough plots of crime data
As a first basic attempt to visualize the data, I plotted a histogram of crime reports, using a binwidth of 30 days.
```{r Exploration - aggregate trend in daily crime over full dataset period with date histogram}
ggplot(crimes, aes(x = date_format)) +
geom_histogram(binwidth = 30)
```
From the histogram, we see indications that crime reports have fallen significantly in Oakland over the last 7 years, as report counts per 30-day period in 2007 and 2008 appear to number between 9,000 and 10,000, while counts per period have been around 4,000 in recent years.
I was also curious how crimes were distributed throughout the day over the period in question. To see the dynamics, I plotted another histogram by time of day, with hourly bins, creating an `hour` variable to more easily bin incidents.
```{r Exploration - crime incidence by time of day (a)}
crimes$time_format <- as.POSIXct(strptime(crimes$Time, "%H:%M:%S"))
#as.POSIXct added to address errors grouping in dplyr
crimes$hour <- as.numeric(format(crimes$time_format, "%H"))
ggplot(crimes, aes(x = hour)) +
geom_histogram(binwidth = 1)
```
The hourly distribution suprised me - first by the huge spike at hour 0 (e.g. midnight to 1am). I suspected that this might be related to data entry issues, so I also checked the composition of crimes that were committed at hour zero, loading them from the original `Time` variable. From this, I discovered that 98,823 of the 118,590 `one_am` records have the value exactly "0:00:00". This strengthened my suspicion about the spike being driven by coding issues.
Relatively confident that most of the 98,823 records occurring at midnight were probably set in the absence of a known time for a crime, or as a result of time recording errors, I decided to run the histogram on a subset that removed the 'exactly midnight' crimes.
```{r Exploration - crime incidence by time of day (b)}
one_am <- subset(crimes, substr(as.character(Time),1,1) == "0" )
summary(one_am$Time)
ggplot(subset(crimes, as.character(Time) != "0:00:00") , aes(x = hour)) +
geom_histogram(binwidth = 1)
```
In the modified histogram, I was also somewhat surprised that crimes don't spike to a greater extent in the evenings, with incident rates staying fairly uniform from 10am to 4pm, then rising to a moderate peak around 7pm and declining from there to a minimum at the 5am hour.
Next, I wanted to explore the distribution of crime by latitude and longitude, with histograms for each variable.
After a first glance at raw plots for each, I realized I needed to remove a few obviously incorrect entries, to arrive at the histograms below, and obtained more useful ranges for a chart by summarizing reasonable values for `Lat`.
```{r Exploration - histograms by longitude and latitude}
summary(subset(crimes$Lat, (crimes$Lat < 40 & crimes$Lat > 37)))
ggplot(subset(crimes, (Lat > 37.7 & Lat < 37.9)), aes(x = Lat)) +
geom_histogram()
summary(subset(crimes$Lng, (crimes$Lng > -123 & crimes$Lng < -122.1 )))
ggplot(subset(crimes, (Lng > -122.35 & Lng < -122.15)), aes(x = Lng)) +
geom_histogram()
crimesMapSubset <- subset(crimes, (Lat > 37.7 & Lat < 37.9) &
(Lng > -122.35 & Lng < -122.15))
```
Crime seems to skew toward the mid-Northern and Western parts of the city, based on the histogram.
For a more detailed view of geographic distribution, I decided to use ggmap to plot a basic heatmap of crime in Oakland for the period in question.
```{r Exploration - crime heatmap by longitude and latitude}
oakland_location <- c(-122.33, 37.71, -122.15, 37.9)
oakland_map <- get_map(location = oakland_location, source="google",
maptype="hybrid")
ggmap(oakland_map) +
geom_density2d(data = crimesMapSubset, aes(x = Lng, y = Lat), size = 1) +
stat_density2d(data = crimesMapSubset, aes(x = Lng, y = Lat,
fill = ..level.., alpha = ..level..),
size = 0.01, bins = 20, geom = 'polygon') +
scale_fill_gradient(low = "green", high = "red") +
guides(fill = guide_legend(position='None'))
```
In the heatmap, we see a strong concentration of crimes in the downtown area, with a pocket also slightly to the northwest along San Pablo Avenue, then all along International Avenue to the southeast.
For another view of Oakland crime dynamics, I created a line plot of incidents per day, using the dplyr aggregation techniques we learned in the Data Analysis in R course.
_Note: creating new dataframe based on incidents per day, using dplyr 'verbose' method, then using this in line plot._
```{r Exploration - line plot of crime incidents per day over time}
crime_day <- group_by(crimes, date_format)
crime_by_day <- summarize(crime_day,
incidents = sum(any_crime),
n=n())
ggplot(aes(x=date_format, y=incidents), data=crime_by_day) +
geom_line() +
geom_smooth()
```
In the daily line plot, the decline of crime reports over time is clearly visible.
As daily crime incident counts are fairly noisy, I wanted to take a look at potentially smoother time increments - creating a line plot of `any_crime` incidents each month.
To create the monthly plot, I cut our `date_format` data in monthly units, using the syntax we learned in lesson 5 of the Data Analysis in R course, then aggregated crime data by month. To do so, I applied dplyr methods again, but this time using the 'concise' syntax.
```{r Exploration - line plot of crime incidents per month over time}
crimes$month <- as.Date(cut(crimes$date_format,
breaks = "month"))
crime_by_month <- crimes %>%
group_by(month) %>%
summarize(incidents = sum(any_crime),
n = n()) %>%
arrange(month)
ggplot(aes(x=month, y=incidents), data=crime_by_month) +
geom_line() +
geom_smooth(aes(group = 1))
```
The downward trend in crime reports is again evident in plotted monthly crime report data. Another striking feature of the chart is the large vertical drop in incident count at the beginning of 2014.
It seems very strange to me that the reported number of crimes would be fairly constant throughout 2012 and 2013, fall by around 40% as the year changed, then persist at a largely constant lower level around 3,700 incidents per month for the next year. I wonder if there was a major change in the way crimes were recorded or reported starting at the beginning of 2014.
To better understand the distribution of daily crime rates, I also plotted mean, median and quartile measures of daily crime rates.
Also - to note - I was seeing several report data points for dates in the future. I assume these were the result of data entry errors. For all plots going forward, I have chosen to subset by incidents occurring prior to a 'cut-off' date of June 2015.
*Mean and median daily crime rates:*
```{r Exploration - line plot of mean and median daily crime rates per month over time (a)}
cutOffDate = '2015-06-01'
crime_by_day <- subset(crime_by_day, date_format < cutOffDate)
crime_by_day$month <- as.Date(cut(crime_by_day$date_format,
breaks = "month"))
daily_by_month <- crime_by_day %>%
group_by(month) %>%
summarize(mean_incidents = mean(incidents),
median_incidents = median(incidents),
bottom_quartile = quantile(incidents, 0.25),
top_quartile = quantile(incidents, 0.75),
bottom_decile = quantile(incidents, 0.1),
top_decile = quantile(incidents, 0.9),
n = n()) %>%
arrange(month)
ggplot(daily_by_month, aes(x=month, y=mean_incidents)) +
geom_line(color = 'blue') +
geom_line(data = daily_by_month, aes(x=month, y=median_incidents, color = 'red')) +
theme(legend.position = "none")
```
### Initial Exploration: Rough plots with incidents segmented by crime type
After having uncovered some interesting insights about local crime trends at an aggregate level, I wanted to dive down into crime segments, using some of the multi-variate visualization techniques we covered in the later lessons of Data Analysis in R
To understand how I could best group crime report incidents by description, I surveyed all the potential values appearing in the columns `CrimeCat` and `Desc`.
I assessed `unique(crimes$Desc)` but suppressed the output for this column because there are about 1800 unique descriptions in the fields. This would not be a useful coulmn to use in grouping for plots.
On the other hand, the `CrimeCat` column includes about 60 unique categories as verified by `unique(crimes$CrimeCat)` This number is too large for tractable visualizations, but I decided I could group these into a smaller number of main categories (variable - `mainCat`) using grepl string matching.
Thus I grouped crimes by homicide, robbery/larceny, assault, rape, weapons, domestic violence, traffic and court viaolations and 'quality of life' (this was a class of crimes noted in the dataset, which included somewhat minor violations including 'curfew-loitering', drug possession and incidents related to public liquor consumption).
```{r Exploration - crime incidents per day grouped by crime type (a)}
crimes <- transform(crimes, mainCat =
ifelse(grepl("HOMICIDE", CrimeCat, ignore.case = T),"homicide",
ifelse(grepl("ROBBERY|LARCENY", CrimeCat, ignore.case = T),"robbery",
ifelse(grepl("ASSAULT", CrimeCat, ignore.case = T),"assault",
ifelse(grepl("WEAPONS", CrimeCat, ignore.case = T),"weapons",
ifelse(grepl("DOM-VIOL", CrimeCat, ignore.case = T),
"domestic_violence",
ifelse(grepl("RAPE", CrimeCat, ignore.case = T),"rape",
ifelse(grepl("TRAFFIC", CrimeCat, ignore.case = T),"traffic",
ifelse(grepl("COURT", CrimeCat, ignore.case = T),"court",
ifelse(grepl("QUALITY|VANDALISM", CrimeCat, ignore.case = T),
"quality_of_life","other"))))))))))
trimmed_crimes_df <- crimes[,c("Lat","Lng","mainCat","date_format", "time_format")]
write.csv(trimmed_crimes_df, file = "oakland_crimes.csv")
getwd()
write.csv(trimmed_crimes_df, file = "oakland_crimes.csv")
